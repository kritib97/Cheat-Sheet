{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Sheet of Important/Common Data Modelling Steps/Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Function to get a coulmn of Monday's date extracted from the week of another Date Column\n",
    "2. Function to find the words with most occurrence in a String type column\n",
    "3. Function to create categorical variables based on most frequency in categorical column(s)\n",
    "4. Function to create a lag of one in a column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to get a coulmn of Monday's date extracted from the week of another Date Column:\n",
    "import pendulum\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_week(dataset, col):\n",
    "    df= dataset\n",
    "    col= col\n",
    "    df['Week'] = df[col].isnull() # Check for missing dates\n",
    "\n",
    "    l1=list()\n",
    "    for i in range(0, len(df)):\n",
    "        if (df['Week'].iloc[i] == False):\n",
    "            j = df[col].iloc[i]\n",
    "            today = pendulum.parse(str(j))\n",
    "            start = today.start_of('week')\n",
    "            l1.append(start)\n",
    "        else:\n",
    "            l1.append(None)\n",
    "    df['Week_start']=l1\n",
    "    \n",
    "    df['Week_start']  = pd.to_datetime(df['Week_start'] ,utc=True, format='%Y-%m-%dT%H:%M:%S+00:00' ,errors = 'coerce')\n",
    "    df['Week_start'] = df['Week_start'].dt.date\n",
    "    #df['Week_start'].value_counts()\n",
    "    df['Week'] = df['Week_start']\n",
    "    \n",
    "    # Instantiate LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df['Week'] = le.fit_transform(df['Week'])\n",
    "\n",
    "    df['Week'] = df['Week'] +1\n",
    "    df['Week'].value_counts()\n",
    "    \n",
    "\n",
    "# Usage of Function:\n",
    "dataset = df_dates\n",
    "col = 'Date_col'\n",
    "create_week(dataset, col)\n",
    "df_dates[['Week', 'Week_start']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function to find the words with most occurrence in a String type column:\n",
    "def find_common_words(dataset, col):\n",
    "    df = dataset\n",
    "    df[col] = df[col].astype(str)\n",
    "    d = df[col].copy()\n",
    "\n",
    "    for i in range(0, len(df)-1):\n",
    "        j = df[col].iloc[i]\n",
    "        d.iloc[i] = j.split() \n",
    "    #print(d)\n",
    "\n",
    "    l=list()\n",
    "    for i in range(0, len(df)-1):\n",
    "        l += d.iloc[i]\n",
    "    #print(l)\n",
    "\n",
    "    #from collections import Counter\n",
    "    from collections import Counter\n",
    "    # Pass the split_it list to instance of Counter class. \n",
    "    Counter = Counter(l)\n",
    "    # most_common() produces k frequently encountered input values and their respective counts. \n",
    "    most_occur = Counter.most_common(100) \n",
    "    print(most_occur)\n",
    "\n",
    "\n",
    "# Usage of function:\n",
    "dataset = df3\n",
    "col = 'String_col'\n",
    "find_common_words(dataset, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function to create categorical variables based on most frequency in categorical column(s):\n",
    "def create_categories(dataset, list_col):\n",
    "    list_col=list_col\n",
    "    df = dataset\n",
    "    for i in list_col:\n",
    "        ag=df.groupby([i])['Number'].agg('count').reset_index()  \n",
    "        ag=ag.sort_values(by='Number', ascending=False)\n",
    "        k=list(ag.columns)\n",
    "        #ag.head(4)\n",
    "    \n",
    "        col = 'New ' + k[0]\n",
    "        df[col] = (df[k[0]].str.contains('|'.join([ag.iloc[0,0], ag.iloc[1,0], ag.iloc[2,0]]), na=False))\n",
    "        df[col] = df[col].astype(object).replace({False: 0, True: 1})\n",
    "        df[col] = df[col].apply(lambda x: 'Other ' + k[0] + 's' if x ==0 else x)\n",
    "        df.loc[df[k[0]] ==ag.iloc[0,0],col] = ag.iloc[0,0]\n",
    "        df.loc[df[k[0]] ==ag.iloc[1,0],col] = ag.iloc[1,0]\n",
    "        df.loc[df[k[0]] ==ag.iloc[2,0],col] = ag.iloc[2,0]\n",
    "        df[k[0]].fillna(\"Null\", inplace = True) \n",
    "        df.loc[df[k[0]]==\"Null\" ,col] = \"Null\"\n",
    "\n",
    "        # Check the categories to ensure nothing is left out\n",
    "        #print(df[col].unique()) \n",
    "        #df[col].value_counts() \n",
    "    \n",
    "        c=df[[k[0], col]]\n",
    "        #print(c[c[col]==1][k[0]].unique())\n",
    "        df.loc[df[col]==1 ,col] = 'Other ' + k[0] + 's'\n",
    "        print(df[col].unique())\n",
    "        #df[col].value_counts()\n",
    "\n",
    "        \n",
    "        \n",
    "#Usage of function:\n",
    "list_col = ['Col1', 'Col2', 'Col3']\n",
    "dataset = df_string_cols\n",
    "create_categories(dataset,list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Function to create a lag of one in a column(s) :\n",
    "def create_lag(dataset, col_list):\n",
    "    df = dataset\n",
    "    for col in col_list:\n",
    "        df['Lag ' + col] = 0\n",
    "        for i in range(1,len(df)):\n",
    "            df['Lag ' + col].iloc[i] =  df[col].iloc[i-1]\n",
    "        del df[col]\n",
    "        \n",
    "#Usage of function:\n",
    "list_col = ['Col1', 'Col2', 'Col3']\n",
    "dataset = df1\n",
    "create_lag(dataset,list_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Recently Used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Random Forest - Confusion Matrix, Variable Importance, Predicted Probabilities\n",
    "2. Logistic Regression -  Confusion Matrix, Coefficients, Concordance/Discordance/Somer's D, AUC\n",
    "3. Logistic Step Regression - Best subset of features, Metric score of best subset, All subsets with corresponding metric value    (metric - Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Random Forest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10,max_depth=3, max_features='sqrt', random_state=10) # max_features: reduces the variation in accuracy for same data, random_state: reduces variation in accuracy for different train-test data\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_pred=clf.predict(X_test)\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix\n",
    "\n",
    "# Metrics of confusion matrix\n",
    "from sklearn import metrics\n",
    "print('Accuracy: ',metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: %.3f' % precision_score(Y_test, Y_pred))\n",
    "print('Recall: %.3f' % recall_score(Y_test, Y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(Y_test, Y_pred))\n",
    "\n",
    "# First 20 important variables of Random Forest\n",
    "# var ipm based on inbuilt function\n",
    "sorted_idx = clf.feature_importances_.argsort()\n",
    "m=pd.DataFrame(X.columns[sorted_idx], clf.feature_importances_[sorted_idx])\n",
    "m.reset_index(level=0, inplace=True)\n",
    "m.rename(columns={'index': 'Importance', 0: 'Variable'}, inplace=True)\n",
    "cols=['Variable','Importance']\n",
    "m = m.reindex(columns=cols)\n",
    "m = m.sort_values(by='Importance', ascending=False)\n",
    "m['Importance'] = m['Importance'].round(decimals=6)\n",
    "iv=m[:20]\n",
    "iv\n",
    "\n",
    "# To get predicted probabilities\n",
    "predictions = clf.predict_proba(X_test) \n",
    "predictions=pd.DataFrame(predictions)\n",
    "predictions.columns=[\"p0\",\"p1\"]\n",
    "p=predictions['p1']\n",
    "p=pd.DataFrame(p,columns=['p1'])\n",
    "p.columns=['p1']\n",
    "Y_test=pd.DataFrame(Y_test)\n",
    "df=pd.concat([Y_test.reset_index(drop=True),p.reset_index(drop=True)],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "lr = LogisticRegression(random_state=0).fit(X_train,Y_train)\n",
    "Y_pred=lr.predict(X_test)\n",
    "\n",
    "confusion_matrix = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Accuracy: ',metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: %.3f' % precision_score(Y_test, Y_pred))\n",
    "print('Recall: %.3f' % recall_score(Y_test, Y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(Y_test, Y_pred))\n",
    "\n",
    "# Coefficients\n",
    "#pd.concat([pd.DataFrame(X.columns,pd.DataFrame(np.transpose(lr.coef_)))],axis = 1)\n",
    "m=pd.DataFrame(np.transpose(lr.coef_))\n",
    "m['Var'] = pd.DataFrame(X.columns)\n",
    "m\n",
    "\n",
    "# Function for concordance and discordance\n",
    "def concordance_discordance(model, data, predictors, outcome, testdata):       \n",
    "        Probability = model.predict_proba(data[predictors])\n",
    "        Probability1 = pd.DataFrame(Probability)\n",
    "        Probability1.columns = ['Prob_0','Prob_1']\n",
    "        TruthTable = pd.merge(data[[outcome]], Probability1[['Prob_1']], how='inner', left_index=True, right_index=True)\n",
    "        zeros = TruthTable[(TruthTable[outcome]==0)].reset_index().drop(['index'], axis = 1)\n",
    "        ones = TruthTable[(TruthTable[outcome]==1)].reset_index().drop(['index'], axis = 1)\n",
    "        from bisect import bisect_left, bisect_right\n",
    "        zeros_list = sorted([zeros.iloc[j,1] for j in zeros.index])\n",
    "        zeros_length = len(zeros_list)\n",
    "        disc = 0\n",
    "        ties = 0\n",
    "        conc = 0\n",
    "        for i in ones.index:\n",
    "            cur_conc = bisect_left(zeros_list, ones.iloc[i,1])\n",
    "            cur_ties = bisect_right(zeros_list, ones.iloc[i,1]) - cur_conc\n",
    "            conc += cur_conc\n",
    "            ties += cur_ties\n",
    "        pairs_tested = zeros_length * len(ones.index)\n",
    "        disc = pairs_tested - conc - ties\n",
    "        print('Pairs = ', pairs_tested)\n",
    "        print('Conc = ', conc)\n",
    "        print('Disc = ', disc)\n",
    "        print('Tied = ', ties)\n",
    "        concordance = round(conc/pairs_tested,2)\n",
    "        discordance = round(disc/pairs_tested,2)\n",
    "        ties_perc = round(ties/pairs_tested,2)\n",
    "        Somers_D = round((conc - disc)/pairs_tested,2)\n",
    "        print('Concordance = ', concordance*100, '%')\n",
    "        print('Discordance = ', discordance*100, '%')\n",
    "        print('Tied = ', ties_perc, '%')\n",
    "        print('Somers D = ', Somers_D)\n",
    "\n",
    "#Usage of above function:\n",
    "x_col_list=X.columns\n",
    "d = concordance_discordance(lr,df,x_col_list, 'Y_col',Y_test)\n",
    "d = pd.DataFrame(d)\n",
    "\n",
    "# AUC for Logistic Regression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lr_probs = df.p1\n",
    "lr_auc = roc_auc_score(df['INC_SAP Occur'], lr_probs)\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Logistic Step Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "# Dependencies:\n",
    "# In case of error while running next code snippet for newer versions of scikit:\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import mlrose\n",
    "import joblib\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "\n",
    "#!pip install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "sfs1 = sfs(lr,k_features='best',forward=True,floating=False, scoring='accuracy',cv=5) # Gives the set of features based on best value of chosen metric\n",
    "sfs1 = sfs1.fit(X_train, Y_train)\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "#print(feat_cols)#to see which columns are selected\n",
    "\n",
    "print(feat_cols)#to see which columns are selected\n",
    "f = feat_cols\n",
    "l = list(sfs1.k_feature_names_)\n",
    "l = pd.DataFrame(l)\n",
    "l\n",
    "\n",
    "sfs1.k_score_  # Prediction (accuracy) score of the optimal set of features\n",
    "\n",
    "s = sfs1.subsets_ # Get all the subsets of features\n",
    "type(s) # dict\n",
    "feat_subset = pd.DataFrame(s)\n",
    "feat_subset\n",
    "\n",
    "# Feed all the features of best subset to logistic regressor:\n",
    "X1=t[['Col1', 'Col2', 'Col5']].copy()\n",
    "Y = t['Y_var'].copy()\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X1,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0).fit(X_train,Y_train)\n",
    "Y_pred=lr.predict(X_test)\n",
    "confusion_matrix = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Accuracy: ',metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: %.3f' % precision_score(Y_test, Y_pred))\n",
    "print('Recall: %.3f' % recall_score(Y_test, Y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(Y_test, Y_pred))\n",
    "\n",
    "# Coefficients\n",
    "#pd.concat([pd.DataFrame(X.columns,pd.DataFrame(np.transpose(lr.coef_)))],axis = 1)\n",
    "m=pd.DataFrame(np.transpose(lr.coef_))\n",
    "m['Var'] = pd.DataFrame(X1.columns)\n",
    "m\n",
    "\n",
    "# For concordance/discordance, use function in Logistic Regression part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column based on conditions in another String type column:\n",
    "df1['Col3'] = (df1['Col1'].str.contains('Hello', na=False)) & (df1['Col2'].str.contains('Hi', na=False))\n",
    "df1['Col3'] = df1['Col3'].astype(object).replace({False: 0, True: 1}) \n",
    "df1['Col3'].value_counts()\n",
    "\n",
    "# Replace a value in an entire column:\n",
    "df3['Col1'] = df3['Col1'].replace(['Hi'],'Hello')\n",
    "\n",
    "# Replace value of a String type column based on a condition on another String type column:\n",
    "df.loc[df['Col1'].str.contains('Hi', na=False), 'Col2'] = 'Hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert a column to datetime\n",
    "df1['Date2']=pd.to_datetime(df2['Date1'], format='%d-%m-%Y')    # 'Format' is the format of the dates present in the columns initially.\n",
    "                                                                # It has to be mentioned, otherwise pd.to_datetime will convert into random formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hour from datetime column:\n",
    "df3['Hour']=df3.Date_column.dt.hour\n",
    "\n",
    "# Create column having part of the day based on hour:\n",
    "df3['Time of Day'] = 0\n",
    "df3.loc[(df3.Hour >= 6) & (df3.Hour < 12), 'Time of Day'] = 'Morning'\n",
    "df3.loc[(df3.Hour >= 12) & (df3.Hour < 18), 'Time of Day'] = 'Afternoon'\n",
    "df3.loc[(df3.Hour >= 18) & (df3.Hour < 24), 'Time of Day'] = 'Evening'\n",
    "df3.loc[df3.Hour < 6 , 'Time of Day'] = 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of business days between two date columns (excluding weekends):\n",
    "from pandas.tseries.offsets import *\n",
    "#pd.date_range('2011-01-05', '2011-01-09', freq=BDay()).shape[0]  ----to get diff in days without weekends for 2 dates\n",
    "df3['No. of Business Days'] = 0\n",
    "### USE LOC TO FIND NON-NULL ROWS\n",
    "df3.loc[(df3.DateA.notnull()) & (df3.DateB.notnull()), 'No. of Business Days'] = df3[(df3.DateA.notnull()) & (df3.DateB.notnull())].apply(lambda row: len(pd.bdate_range(row['DateA'], row['DateB'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables:\n",
    "df3=pd.merge(df1,df2,how='left', on= 'Common_column')   # how = inner/outer/left/right\n",
    "\n",
    "# Concat tables along columns:\n",
    "df = pd.concat([df1,df2,df3], axis=1)   # axis=0 to concat along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find correlation between input variable columns and output variable column:\n",
    "# Inbuilt pandas function corrwith to get all correlation between independent vars and dependent var\n",
    "X = df.drop(['Y_var'], axis=1)\n",
    "Y = df['Y_var'].copy()\n",
    "cor=X.corrwith(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff of no. of events between present & the last row:\n",
    "df['Increase_in_count'] = df['No. of events'].diff(periods = 1)  # If periods=2, then finds difference between last and last-2 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical columns to Dummy variables using the categories in it:\n",
    "x = df[['col1','col2','col3','col4']].copy()\n",
    "x1=pd.get_dummies(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a substring preceding all column names of a dataframe:\n",
    "col = list(df.columns)\n",
    "col = ['A-Z' + sub for sub in col]\n",
    "df.columns = col\n",
    "\n",
    "# Reset index of dataframe:\n",
    "df.reset_index(level=1, inplace=True)\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
