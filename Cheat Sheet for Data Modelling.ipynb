{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Sheet of Important/Common Data Modelling Steps/Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Function to get a coulmn of Monday's date extracted from the week of another Date Column <br />\n",
    "2. Function to find the words with most occurrence in a String type column <br />\n",
    "3. Function to create categorical variables based on most frequency in categorical column(s) <br />\n",
    "4. Function to create a lag of one in a column(s) <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to get a coulmn of Monday's date extracted from the week of another Date Column:\n",
    "import pendulum\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def create_week(dataset, col):\n",
    "    df= dataset\n",
    "    col= col\n",
    "    df['Week'] = df[col].isnull() # Check for missing dates\n",
    "\n",
    "    l1=list()\n",
    "    for i in range(0, len(df)):\n",
    "        if (df['Week'].iloc[i] == False):\n",
    "            j = df[col].iloc[i]\n",
    "            today = pendulum.parse(str(j))\n",
    "            start = today.start_of('week')\n",
    "            l1.append(start)\n",
    "        else:\n",
    "            l1.append(None)\n",
    "    df['Week_start']=l1\n",
    "    \n",
    "    df['Week_start']  = pd.to_datetime(df['Week_start'] ,utc=True, format='%Y-%m-%dT%H:%M:%S+00:00' ,errors = 'coerce')\n",
    "    df['Week_start'] = df['Week_start'].dt.date\n",
    "    #df['Week_start'].value_counts()\n",
    "    df['Week'] = df['Week_start']\n",
    "    \n",
    "    # Instantiate LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df['Week'] = le.fit_transform(df['Week'])\n",
    "\n",
    "    df['Week'] = df['Week'] +1\n",
    "    df['Week'].value_counts()\n",
    "    \n",
    "\n",
    "# Usage of Function:\n",
    "dataset = df_dates\n",
    "col = 'Date_col'\n",
    "create_week(dataset, col)\n",
    "df_dates[['Week', 'Week_start']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function to find the words with most occurrence in a String type column:\n",
    "def find_common_words(dataset, col):\n",
    "    df = dataset\n",
    "    df[col] = df[col].astype(str)\n",
    "    d = df[col].copy()\n",
    "\n",
    "    for i in range(0, len(df)-1):\n",
    "        j = df[col].iloc[i]\n",
    "        d.iloc[i] = j.split() \n",
    "    #print(d)\n",
    "\n",
    "    l=list()\n",
    "    for i in range(0, len(df)-1):\n",
    "        l += d.iloc[i]\n",
    "    #print(l)\n",
    "\n",
    "    #from collections import Counter\n",
    "    from collections import Counter\n",
    "    # Pass the split_it list to instance of Counter class. \n",
    "    Counter = Counter(l)\n",
    "    # most_common() produces k frequently encountered input values and their respective counts. \n",
    "    most_occur = Counter.most_common(100) \n",
    "    print(most_occur)\n",
    "\n",
    "\n",
    "# Usage of function:\n",
    "dataset = df3\n",
    "col = 'String_col'\n",
    "find_common_words(dataset, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Function to create categorical variables based on most frequency in categorical column(s):\n",
    "def create_categories(dataset, list_col):\n",
    "    list_col=list_col\n",
    "    df = dataset\n",
    "    for i in list_col:\n",
    "        ag=df.groupby([i])['Number'].agg('count').reset_index()  \n",
    "        ag=ag.sort_values(by='Number', ascending=False)\n",
    "        k=list(ag.columns)\n",
    "        #ag.head(4)\n",
    "    \n",
    "        col = 'New ' + k[0]\n",
    "        df[col] = (df[k[0]].str.contains('|'.join([ag.iloc[0,0], ag.iloc[1,0], ag.iloc[2,0]]), na=False))\n",
    "        df[col] = df[col].astype(object).replace({False: 0, True: 1})\n",
    "        df[col] = df[col].apply(lambda x: 'Other ' + k[0] + 's' if x ==0 else x)\n",
    "        df.loc[df[k[0]] ==ag.iloc[0,0],col] = ag.iloc[0,0]\n",
    "        df.loc[df[k[0]] ==ag.iloc[1,0],col] = ag.iloc[1,0]\n",
    "        df.loc[df[k[0]] ==ag.iloc[2,0],col] = ag.iloc[2,0]\n",
    "        df[k[0]].fillna(\"Null\", inplace = True) \n",
    "        df.loc[df[k[0]]==\"Null\" ,col] = \"Null\"\n",
    "\n",
    "        # Check the categories to ensure nothing is left out\n",
    "        #print(df[col].unique()) \n",
    "        #df[col].value_counts() \n",
    "    \n",
    "        c=df[[k[0], col]]\n",
    "        #print(c[c[col]==1][k[0]].unique())\n",
    "        df.loc[df[col]==1 ,col] = 'Other ' + k[0] + 's'\n",
    "        print(df[col].unique())\n",
    "        #df[col].value_counts()\n",
    "\n",
    "        \n",
    "        \n",
    "#Usage of function:\n",
    "list_col = ['Col1', 'Col2', 'Col3']\n",
    "dataset = df_string_cols\n",
    "create_categories(dataset,list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Function to create a lag of one in a column(s) :\n",
    "def create_lag(dataset, col_list):\n",
    "    df = dataset\n",
    "    for col in col_list:\n",
    "        df['Lag ' + col] = 0\n",
    "        for i in range(1,len(df)):\n",
    "            df['Lag ' + col].iloc[i] =  df[col].iloc[i-1]\n",
    "        del df[col]\n",
    "        \n",
    "#Usage of function:\n",
    "list_col = ['Col1', 'Col2', 'Col3']\n",
    "dataset = df1\n",
    "create_lag(dataset,list_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Recently Used"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Random Forest - Confusion Matrix, Variable Importance, Predicted Probabilities <br />\n",
    "2. Logistic Regression -  Confusion Matrix, Coefficients, Concordance/Discordance/Somer's D, AUC <br />\n",
    "3. Logistic Step Regression - Best subset of features, Metric score of best subset, All subsets with corresponding metric value    (metric - Accuracy) <br />\n",
    "4. KMeans - Knee Locator (Elbow method), inertia, cluster labels, scatter-plot, silhouette score and graph <br />\n",
    "5. Decision Tree - Accuracy, Text & Image representations <br />\n",
    "6. Time Series - Unobserved Components algorithm, Mean Squared Error, Correlation <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Random Forest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10,max_depth=3, max_features='sqrt', random_state=10) # max_features: reduces the variation in accuracy for same data, random_state: reduces variation in accuracy for different train-test data\n",
    "clf.fit(X_train,Y_train)\n",
    "Y_pred=clf.predict(X_test)\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix\n",
    "\n",
    "# Metrics of confusion matrix\n",
    "from sklearn import metrics\n",
    "print('Accuracy: ',metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: %.3f' % precision_score(Y_test, Y_pred))\n",
    "print('Recall: %.3f' % recall_score(Y_test, Y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(Y_test, Y_pred))\n",
    "\n",
    "# First 20 important variables of Random Forest\n",
    "# var ipm based on inbuilt function\n",
    "sorted_idx = clf.feature_importances_.argsort()\n",
    "m=pd.DataFrame(X.columns[sorted_idx], clf.feature_importances_[sorted_idx])\n",
    "m.reset_index(level=0, inplace=True)\n",
    "m.rename(columns={'index': 'Importance', 0: 'Variable'}, inplace=True)\n",
    "cols=['Variable','Importance']\n",
    "m = m.reindex(columns=cols)\n",
    "m = m.sort_values(by='Importance', ascending=False)\n",
    "m['Importance'] = m['Importance'].round(decimals=6)\n",
    "iv=m[:20]\n",
    "iv\n",
    "\n",
    "# To get predicted probabilities\n",
    "predictions = clf.predict_proba(X_test) \n",
    "predictions=pd.DataFrame(predictions)\n",
    "predictions.columns=[\"p0\",\"p1\"]\n",
    "p=predictions['p1']\n",
    "p=pd.DataFrame(p,columns=['p1'])\n",
    "p.columns=['p1']\n",
    "Y_test=pd.DataFrame(Y_test)\n",
    "df=pd.concat([Y_test.reset_index(drop=True),p.reset_index(drop=True)],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "lr = LogisticRegression(random_state=0).fit(X_train,Y_train)\n",
    "Y_pred=lr.predict(X_test)\n",
    "\n",
    "confusion_matrix = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Accuracy: ',metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: %.3f' % precision_score(Y_test, Y_pred))\n",
    "print('Recall: %.3f' % recall_score(Y_test, Y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(Y_test, Y_pred))\n",
    "\n",
    "# Coefficients\n",
    "#pd.concat([pd.DataFrame(X.columns,pd.DataFrame(np.transpose(lr.coef_)))],axis = 1)\n",
    "m=pd.DataFrame(np.transpose(lr.coef_))\n",
    "m['Var'] = pd.DataFrame(X.columns)\n",
    "m\n",
    "\n",
    "# Function for concordance and discordance\n",
    "def concordance_discordance(model, data, predictors, outcome, testdata):       \n",
    "        Probability = model.predict_proba(data[predictors])\n",
    "        Probability1 = pd.DataFrame(Probability)\n",
    "        Probability1.columns = ['Prob_0','Prob_1']\n",
    "        TruthTable = pd.merge(data[[outcome]], Probability1[['Prob_1']], how='inner', left_index=True, right_index=True)\n",
    "        zeros = TruthTable[(TruthTable[outcome]==0)].reset_index().drop(['index'], axis = 1)\n",
    "        ones = TruthTable[(TruthTable[outcome]==1)].reset_index().drop(['index'], axis = 1)\n",
    "        from bisect import bisect_left, bisect_right\n",
    "        zeros_list = sorted([zeros.iloc[j,1] for j in zeros.index])\n",
    "        zeros_length = len(zeros_list)\n",
    "        disc = 0\n",
    "        ties = 0\n",
    "        conc = 0\n",
    "        for i in ones.index:\n",
    "            cur_conc = bisect_left(zeros_list, ones.iloc[i,1])\n",
    "            cur_ties = bisect_right(zeros_list, ones.iloc[i,1]) - cur_conc\n",
    "            conc += cur_conc\n",
    "            ties += cur_ties\n",
    "        pairs_tested = zeros_length * len(ones.index)\n",
    "        disc = pairs_tested - conc - ties\n",
    "        print('Pairs = ', pairs_tested)\n",
    "        print('Conc = ', conc)\n",
    "        print('Disc = ', disc)\n",
    "        print('Tied = ', ties)\n",
    "        concordance = round(conc/pairs_tested,2)\n",
    "        discordance = round(disc/pairs_tested,2)\n",
    "        ties_perc = round(ties/pairs_tested,2)\n",
    "        Somers_D = round((conc - disc)/pairs_tested,2)\n",
    "        print('Concordance = ', concordance*100, '%')\n",
    "        print('Discordance = ', discordance*100, '%')\n",
    "        print('Tied = ', ties_perc, '%')\n",
    "        print('Somers D = ', Somers_D)\n",
    "\n",
    "#Usage of above function:\n",
    "x_col_list=X.columns\n",
    "d = concordance_discordance(lr,df,x_col_list, 'Y_col',Y_test)\n",
    "d = pd.DataFrame(d)\n",
    "\n",
    "# AUC for Logistic Regression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lr_probs = df.p1\n",
    "lr_auc = roc_auc_score(df['INC_SAP Occur'], lr_probs)\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Logistic Step Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "# Dependencies:\n",
    "# In case of error while running next code snippet for newer versions of scikit:\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "import mlrose\n",
    "import joblib\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "\n",
    "#!pip install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "sfs1 = sfs(lr,k_features='best',forward=True,floating=False, scoring='accuracy',cv=5) # Gives the set of features based on best value of chosen metric\n",
    "sfs1 = sfs1.fit(X_train, Y_train)\n",
    "feat_cols = list(sfs1.k_feature_idx_)\n",
    "#print(feat_cols)#to see which columns are selected\n",
    "\n",
    "print(feat_cols)#to see which columns are selected\n",
    "f = feat_cols\n",
    "l = list(sfs1.k_feature_names_)\n",
    "l = pd.DataFrame(l)\n",
    "l\n",
    "\n",
    "sfs1.k_score_  # Prediction (accuracy) score of the optimal set of features\n",
    "\n",
    "s = sfs1.subsets_ # Get all the subsets of features\n",
    "type(s) # dict\n",
    "feat_subset = pd.DataFrame(s)\n",
    "feat_subset\n",
    "\n",
    "# Feed all the features of best subset to logistic regressor:\n",
    "X1=t[['Col1', 'Col2', 'Col5']].copy()\n",
    "Y = t['Y_var'].copy()\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X1,Y,test_size=0.20,random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0).fit(X_train,Y_train)\n",
    "Y_pred=lr.predict(X_test)\n",
    "confusion_matrix = pd.crosstab(Y_test, Y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Accuracy: ',metrics.accuracy_score(Y_test, Y_pred))\n",
    "# Finding precision and recall\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print('Precision: %.3f' % precision_score(Y_test, Y_pred))\n",
    "print('Recall: %.3f' % recall_score(Y_test, Y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(Y_test, Y_pred))\n",
    "\n",
    "# Coefficients\n",
    "#pd.concat([pd.DataFrame(X.columns,pd.DataFrame(np.transpose(lr.coef_)))],axis = 1)\n",
    "m=pd.DataFrame(np.transpose(lr.coef_))\n",
    "m['Var'] = pd.DataFrame(X1.columns)\n",
    "m\n",
    "\n",
    "# For concordance/discordance, use function in Logistic Regression part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator  #pip install kneed\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the dataframe\n",
    "features = df.copy()\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Get the optimal initial number of clusters (k)\n",
    "kmeans_kwargs = {\n",
    "        \"init\": \"random\",\n",
    "        \"n_init\": 10,\n",
    "        \"max_iter\": 300,\n",
    "        \"random_state\": 42 }\n",
    "# A list holds the SSE (Sum of Squared Errors) values for each k (randomly select k=50)\n",
    "sse = []\n",
    "for k in range(1, 50):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse.append(kmeans.inertia_)\n",
    "# Plot the elbow graph\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 50), sse)\n",
    "plt.xticks(range(1, 50))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()    \n",
    "# Get the optimal value of k\n",
    "kl = KneeLocator(range(1, 50), sse, curve=\"convex\", direction=\"decreasing\" )\n",
    "kl.elbow\n",
    "\n",
    "# Fit the kmeans model using the optimal k/selected k\n",
    "kmeans = KMeans( init=\"random\",n_clusters=5,n_init=10,max_iter=300,random_state=42)\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "# The lowest SSE value\n",
    "kmeans.inertia_\n",
    "\n",
    "# The number of iterations required to converge\n",
    "kmeans.n_iter_\n",
    "\n",
    "# Gets cluster label of each point/row\n",
    "kmeans.labels_\n",
    "\n",
    "# Get indices of points for each cluster\n",
    "{i: np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    "\n",
    "# Add cluster label as a column to the data frame and convert to dummies for further analysis\n",
    "t= inct.copy()\n",
    "df['Y_var'] = main_df['Y_var'].copy()\n",
    "df['Cluster'] = kmeans.labels_\n",
    "clust = df[['Cluster']].copy()\n",
    "clust = pd.get_dummies(clust['Cluster'])\n",
    "col = list(clust.columns)\n",
    "col = ['Cluster_' + str(sub) for sub in col]\n",
    "clust.columns = col\n",
    "clust\n",
    "\n",
    "# Plot the data and cluster silhouette comparison\n",
    "fig, ax1 = plt.subplots(1, figsize=(8, 6), sharex=True, sharey=True) # subplot = 1\n",
    "#fig.suptitle(f\"Clustering Algorithm KMeans Crescents\", fontsize=16)\n",
    "fte_colors = {0: \"red\", 1: \"blue\", 2: \"yellow\", 3: \"green\", 4: \"black\"} # number of colours must be equal to number of clusters k\n",
    "# The k-means plot\n",
    "km_colors = [fte_colors[label] for label in kmeans.labels_]\n",
    "ax1.scatter(scaled_features[:, 0], scaled_features[:, 1], c=km_colors)\n",
    "ax1.set_title(f\"k-means\\nSilhouette: {kmeans_silhouette}\", fontdict={\"fontsize\": 12})\n",
    "\n",
    "# Labeling the clusters\n",
    "centers = kmeans.cluster_centers_\n",
    "# Draw white circles at cluster centers\n",
    "ax1.scatter(centers[:, 0], centers[:, 1], marker='o',c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "for i, c in enumerate(centers):\n",
    "    ax1.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,s=50, edgecolor='k')\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "# Plot the scatter plot of clusters and silhouette plot\n",
    "cluster_labels = kmeans.fit_predict(scaled_features) # This gives a perspective into the density and separation of the formed clusters\n",
    "\n",
    "# Silhouette score of the Kmeans model:\n",
    "silhouette_avg = silhouette_score(scaled_features, cluster_labels) \n",
    "#print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    " # Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(scaled_features, cluster_labels)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, figsize=(8, 6), sharex=True, sharey=True)\n",
    "y_lower = 10\n",
    "for i in range(kmeans.n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / kmeans.n_clusters)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),0, ith_cluster_silhouette_values,facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "# Get silhouette values for all records\n",
    "f = pd.DataFrame(list(sample_silhouette_values))\n",
    "f['Cluster'] = list(kmeans.labels_)\n",
    "f\n",
    "\n",
    "# Compute the silhouette scores for each algorithm\n",
    "from sklearn.metrics import silhouette_score\n",
    "kmeans_silhouette = silhouette_score(scaled_features, kmeans.labels_).round(2)\n",
    "kmeans_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Decision Tree\n",
    "# Split dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Representation of Decisison Tree in the form of text\n",
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)\n",
    "\n",
    "# Representation of Decision Tree in the form of image\n",
    "#conda install python-graphviz\n",
    "#pip install six\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "#from sklearn.externals.six import StringIO   ### Has dependency issues based on version of package ###\n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=\"tree.dot\",  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = X.columns,class_names=['0','1'])\n",
    " \n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#graph.write_png('SAP.png')\n",
    "#Image(graph.create_png())\n",
    "\n",
    "!dot -Tpng tree.dot -o tree.png -Gdpi=600\n",
    "\n",
    "Image(filename = 'tree.png') # Image stored in the file being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Time Series - Unobserved Components\n",
    "df = features.copy()\n",
    "train = df.drop(['Y_var'],axis=1)\n",
    "test=pd.DataFrame(df['Y_var'].copy())\n",
    "\n",
    "train1=train[0:90]\n",
    "train2 = train[90:]\n",
    "\n",
    "test1=test[:90]\n",
    "test2=test[90:]\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "\n",
    "#multi variate\n",
    "unrestricted_model = {'level':'random trend', 'cycle': False, 'damped_cycle': False, 'stochastic_cycle': True,'autoregressive':2}\n",
    "\n",
    "model = sm.tsa.UnobservedComponents(endog=test1, exog=train1, **unrestricted_model)\n",
    "model_fit=model.fit(method='bfgs',disp=False)\n",
    "\n",
    "pred = model_fit.predict(dynamic=True,exog=train2, start = len(train1), end =len(train1) + len(train2)-1)\n",
    "pred\n",
    "\n",
    "# Mean Squared Error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error \n",
    "mse=sqrt(mean_squared_error(pred, test2))\n",
    "print('Mean Squared Error: ' + str(mse))\n",
    "\n",
    "# get correlation between all independent vars and dependent var\n",
    "X = df.drop(['Y_var'], axis=1)\n",
    "Y = df['Y_var'].copy()\n",
    "cor=X.corrwith(Y)\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column based on conditions in another String type column:\n",
    "df1['Col3'] = (df1['Col1'].str.contains('Hello', na=False)) & (df1['Col2'].str.contains('Hi', na=False))\n",
    "df1['Col3'] = df1['Col3'].astype(object).replace({False: 0, True: 1}) \n",
    "df1['Col3'].value_counts()\n",
    "\n",
    "# Replace a value in an entire column:\n",
    "df3['Col1'] = df3['Col1'].replace(['Hi'],'Hello')\n",
    "\n",
    "# Replace value of a String type column based on a condition on another String type column:\n",
    "df.loc[df['Col1'].str.contains('Hi', na=False), 'Col2'] = 'Hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert a column to datetime\n",
    "df1['Date2']=pd.to_datetime(df2['Date1'], format='%d-%m-%Y')    # 'Format' is the format of the dates present in the columns initially.\n",
    "                                                                # It has to be mentioned, otherwise pd.to_datetime will convert into random formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hour from datetime column:\n",
    "df3['Hour']=df3.Date_column.dt.hour\n",
    "\n",
    "# Create column having part of the day based on hour:\n",
    "df3['Time of Day'] = 0\n",
    "df3.loc[(df3.Hour >= 6) & (df3.Hour < 12), 'Time of Day'] = 'Morning'\n",
    "df3.loc[(df3.Hour >= 12) & (df3.Hour < 18), 'Time of Day'] = 'Afternoon'\n",
    "df3.loc[(df3.Hour >= 18) & (df3.Hour < 24), 'Time of Day'] = 'Evening'\n",
    "df3.loc[df3.Hour < 6 , 'Time of Day'] = 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of business days between two date columns (excluding weekends):\n",
    "from pandas.tseries.offsets import *\n",
    "#pd.date_range('2011-01-05', '2011-01-09', freq=BDay()).shape[0]  ----to get diff in days without weekends for 2 dates\n",
    "df3['No. of Business Days'] = 0\n",
    "### USE LOC TO FIND NON-NULL ROWS\n",
    "df3.loc[(df3.DateA.notnull()) & (df3.DateB.notnull()), 'No. of Business Days'] = df3[(df3.DateA.notnull()) & (df3.DateB.notnull())].apply(lambda row: len(pd.bdate_range(row['DateA'], row['DateB'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables:\n",
    "df3=pd.merge(df1,df2,how='left', on= 'Common_column')   # how = inner/outer/left/right\n",
    "\n",
    "# Concat tables along columns:\n",
    "df = pd.concat([df1,df2,df3], axis=1)   # axis=0 to concat along rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find correlation between input variable columns and output variable column:\n",
    "# Inbuilt pandas function corrwith to get all correlation between independent vars and dependent var\n",
    "X = df.drop(['Y_var'], axis=1)\n",
    "Y = df['Y_var'].copy()\n",
    "cor=X.corrwith(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff of no. of events between present & the last row:\n",
    "df['Increase_in_count'] = df['No. of events'].diff(periods = 1)  # If periods=2, then finds difference between last and last-2 events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Categorical columns to Dummy variables using the categories in it:\n",
    "x = df[['col1','col2','col3','col4']].copy()\n",
    "x1=pd.get_dummies(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a substring preceding all column names of a dataframe:\n",
    "col = list(df.columns)\n",
    "col = ['A-Z' + sub for sub in col]\n",
    "df.columns = col\n",
    "\n",
    "# Reset index of dataframe:\n",
    "df.reset_index(level=1, inplace=True)\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
